{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bbdf59a-c2e5-464d-b112-f5a35bf9926d",
   "metadata": {},
   "source": [
    "## 00. PyTorch Fundamentals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69417687-f065-4d01-a65b-ad5378290756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pylt\n",
    "import pandas\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26eb6c5-4824-462a-b25b-de0cf2364c4f",
   "metadata": {},
   "source": [
    "### Introduction to tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34b858f-c65b-411b-a231-be13b8397a3e",
   "metadata": {},
   "source": [
    "## scalar\n",
    "*scalar is a single number and in tensor-speak it's a zero dimension tensor.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc965071-e15a-4acd-932f-7a2866f719e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value of the tensor: 7\n",
      "Dimensions of the tensor: 0\n"
     ]
    }
   ],
   "source": [
    "scalar = torch.tensor(7)\n",
    "print(f'value of the tensor: {scalar.item()}')\n",
    "print(f'Dimensions of the tensor: {scalar.ndim}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ee8ccd-1dfc-4b1f-945c-5e11946424e5",
   "metadata": {},
   "source": [
    "### Vector\n",
    "\n",
    "A vector is a single dimension tensor but can contain many numbers.\n",
    "\n",
    "As in, you could have a vector [3, 2] to describe [bedrooms, bathrooms] in your house. Or you could have [3, 2, 2] to describe [bedrooms, bathrooms, car_parks] in your house.\n",
    "\n",
    "The important trend here is that a vector is flexible in what it can represent (the same with tensors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c6b2794-c4ef-4bf1-a045-9ff1bcde9256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of the vector: 1\n",
      "Shape of the vector: torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "vector = torch.tensor([8, 7])\n",
    "print(f'Dimension of the vector: {vector.ndim}')\n",
    "print(f'Shape of the vector: {vector.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1684f6-d841-47a7-a454-1557835997e8",
   "metadata": {},
   "source": [
    "### Matrix \n",
    "\n",
    "Matrices are as flexible as vectors, except they've got an extra dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae54b6b9-7dc3-4712-894e-0167f0320a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of the vector: 2\n",
      "Shape of the vector: torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "MATRIX = torch.tensor([[7, 9], [0, 9]])\n",
    "\n",
    "print(f'Dimension of the vector: {MATRIX.ndim}')\n",
    "print(f'Shape of the vector: {MATRIX.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cd21d9e-e320-4381-86d7-fac680dd23f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of the vector: 3\n",
      "Shape of the vector: torch.Size([1, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "# Tensor\n",
    "TENSOR = torch.tensor([[[1, 2, 3],\n",
    "                        [3, 6, 9],\n",
    "                        [2, 4, 5]]])\n",
    "print(f'Dimension of the vector: {TENSOR.ndim}')\n",
    "print(f'Shape of the vector: {TENSOR.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a78b4b-42f0-45e4-8511-1c57865568fa",
   "metadata": {},
   "source": [
    "# LETS SUMMARIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6ccb5b-0673-4fdd-8d87-f1f18892fa99",
   "metadata": {},
   "source": [
    "| Name    | What is it?                                                                 | Number of dimensions                                                | Lower or Upper (usually/example) |\n",
    "|---------|-----------------------------------------------------------------------------|----------------------------------------------------------------------|----------------------------------|\n",
    "| Scalar  | A single number                                                             | 0                                                                    | Lower (a)                        |\n",
    "| Vector  | A number with direction (e.g. wind speed with direction) or an array of numbers | 1                                                                    | Lower (y)                        |\n",
    "| Matrix  | A 2-dimensional array of numbers                                            | 2                                                                    | Upper (Q)                        |\n",
    "| Tensor  | An n-dimensional array of numbers (0D = scalar, 1D = vector, etc.)          | Can be any number of dimensions                                      | Upper                            |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fdff56-e1ba-4414-b1fa-67bba9039fa9",
   "metadata": {},
   "source": [
    "# Random Tensors\n",
    "\n",
    "We've established that **tensors represent some form of data**.  \n",
    "\n",
    "Machine learning models such as neural networks manipulate and seek patterns within tensors.  \n",
    "\n",
    "However, when building models with **PyTorch**, it's rare to create tensors by hand (like we've been doing so far).  \n",
    "\n",
    "Instead, a machine learning model often starts out with **large random tensors of numbers** and adjusts these random numbers as it works through data to better represent it.  \n",
    "\n",
    "---\n",
    "\n",
    "### In essence:\n",
    "1. Start with random numbers  \n",
    "2. Look at data  \n",
    "3. Update random numbers  \n",
    "4. Look at data  \n",
    "5. Update random numbers...  \n",
    "\n",
    "---\n",
    "\n",
    "As a data scientist, you define how the machine learning model:  \n",
    "- **Initializes** → how it starts with random numbers  \n",
    "- **Represents data** → how it looks at data  \n",
    "- **Optimizes** → how it updates the random numbers  \n",
    "\n",
    "---\n",
    "\n",
    "### Creating Random Tensors in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f60b6aaf-3cec-4c01-9c8b-db46b570fb1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the tensor: torch.Size([4, 4, 9])\n",
      "Data type: torch.float32 \n",
      "\n",
      "tensor([[[0.0504, 0.1529, 0.7912, 0.7139, 0.3253, 0.7680, 0.7462, 0.4134,\n",
      "          0.4700],\n",
      "         [0.3667, 0.0061, 0.6525, 0.4448, 0.3475, 0.7442, 0.3174, 0.9970,\n",
      "          0.5822],\n",
      "         [0.0468, 0.0233, 0.1331, 0.3126, 0.8998, 0.0353, 0.0408, 0.1027,\n",
      "          0.0201],\n",
      "         [0.3672, 0.2071, 0.7648, 0.5944, 0.5853, 0.9856, 0.2334, 0.4265,\n",
      "          0.7335]],\n",
      "\n",
      "        [[0.0793, 0.5394, 0.9670, 0.3694, 0.2816, 0.6009, 0.7456, 0.7286,\n",
      "          0.4025],\n",
      "         [0.1240, 0.7781, 0.2448, 0.7096, 0.2484, 0.4169, 0.4272, 0.9825,\n",
      "          0.4164],\n",
      "         [0.9887, 0.2832, 0.6494, 0.7354, 0.1801, 0.2302, 0.0594, 0.9070,\n",
      "          0.3206],\n",
      "         [0.6450, 0.1805, 0.6504, 0.2554, 0.5649, 0.5604, 0.9675, 0.3219,\n",
      "          0.9737]],\n",
      "\n",
      "        [[0.2475, 0.2523, 0.2507, 0.3250, 0.8399, 0.8780, 0.9019, 0.3290,\n",
      "          0.7969],\n",
      "         [0.0798, 0.7652, 0.3747, 0.3051, 0.3304, 0.7422, 0.8784, 0.5432,\n",
      "          0.6726],\n",
      "         [0.8198, 0.5891, 0.4644, 0.1755, 0.2498, 0.3662, 0.7905, 0.3062,\n",
      "          0.4041],\n",
      "         [0.0571, 0.5070, 0.0267, 0.9868, 0.8480, 0.6474, 0.6276, 0.9090,\n",
      "          0.3744]],\n",
      "\n",
      "        [[0.2904, 0.0219, 0.6768, 0.0461, 0.6027, 0.8705, 0.7668, 0.9233,\n",
      "          0.3267],\n",
      "         [0.8451, 0.5009, 0.2321, 0.9575, 0.7396, 0.9864, 0.8863, 0.4852,\n",
      "          0.2814],\n",
      "         [0.2530, 0.9222, 0.4932, 0.8491, 0.7398, 0.6673, 0.6129, 0.6995,\n",
      "          0.1432],\n",
      "         [0.1730, 0.5122, 0.9338, 0.1834, 0.8310, 0.5831, 0.2609, 0.8133,\n",
      "          0.1848]]])\n"
     ]
    }
   ],
   "source": [
    "random_tensor = torch.rand(size=(4, 4, 9))\n",
    "print(f'Shape of the tensor: {random_tensor.shape}')\n",
    "print(f'Data type: {random_tensor.dtype} \\n')\n",
    "\n",
    "print(random_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3393bf0e-e4d0-4ea1-86e7-e00a1cb59def",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]]),\n",
       " torch.float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a tensor of all zeros\n",
    "zeros = torch.zeros(size=(3, 4))\n",
    "zeros, zeros.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56d86b26-cd30-4420-a1c5-68bdf13f05f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]]),\n",
       " torch.float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a tensor of all ones\n",
    "ones = torch.ones(size=(3, 4))\n",
    "ones, ones.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93021483-f088-40d5-9843-bdc18f930785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero_to_ten_deprecated = torch.range(0, .9) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0db904c-fdab-4134-ae16-24f41a13a40e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_to_ten = torch.arange(start=0, end=11, step=1)\n",
    "zero_to_ten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d88d2ae8-cf65-4df4-91e5-6226a6523f25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zeros_like  = torch.zeros_like(input=zero_to_ten)\n",
    "zeros_like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8345ab18-80b5-4c66-8af1-9b277434c555",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float_32_tensor = torch.tensor([3, 6, 9], \n",
    "                            dtype=None,\n",
    "                              device=None,\n",
    "                              requires_grad=False)\n",
    "\n",
    "float_32_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c39f4d-e579-44a5-800f-d0ec9f41cc5d",
   "metadata": {},
   "source": [
    "# Tensor Datatypes in PyTorch\n",
    "\n",
    "Tensors in PyTorch can be created with **different datatypes** (`dtype`). The datatype determines:\n",
    "\n",
    "* **How the tensor stores numbers** (integers, floats, booleans, etc.)\n",
    "* **The precision of numbers** (8-bit, 16-bit, 32-bit, 64-bit).\n",
    "* **The device they run on** (CPU or GPU).\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 Why so many datatypes?\n",
    "\n",
    "1. **Precision (detail of numbers)**\n",
    "\n",
    "   * `torch.float16` (half precision) uses **less memory**, runs **faster**, but is **less accurate**.\n",
    "   * `torch.float32` (single precision) is the **default**, balancing speed and accuracy.\n",
    "   * `torch.float64` (double precision) is **more accurate**, but **slower** and uses **more memory**.\n",
    "\n",
    "2. **Device compatibility**\n",
    "\n",
    "   * If you see **`torch.cuda`**, the tensor is using the **GPU**.\n",
    "   * If no device is set, tensors are created on the **CPU** by default.\n",
    "\n",
    "3. **Operations compatibility**\n",
    "\n",
    "   * Tensors must have the **same datatype** and be on the **same device** for operations.\n",
    "   * Example: `float32` tensor cannot directly be added to a `float16` tensor without casting.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 Summary Table of PyTorch Datatypes\n",
    "\n",
    "| **Datatype**     | **Alias**      | **Size** | **Description**                                  | **Use case**                                   |\n",
    "| ---------------- | -------------- | -------- | ------------------------------------------------ | ---------------------------------------------- |\n",
    "| `torch.float16`  | `torch.half`   | 16-bit   | Low precision float (half precision)             | Faster training (e.g. mixed precision on GPUs) |\n",
    "| `torch.float32`  | `torch.float`  | 32-bit   | Default float (single precision)                 | Standard ML/DL computations                    |\n",
    "| `torch.float64`  | `torch.double` | 64-bit   | High precision float (double precision)          | Scientific computing, when accuracy > speed    |\n",
    "| `torch.int8`     | -              | 8-bit    | Integer                                          | Compact storage, quantized models              |\n",
    "| `torch.int16`    | `torch.short`  | 16-bit   | Small integer                                    | Rarely used                                    |\n",
    "| `torch.int32`    | `torch.int`    | 32-bit   | Standard integer                                 | General integer ops                            |\n",
    "| `torch.int64`    | `torch.long`   | 64-bit   | Large integer                                    | Indexing, counters                             |\n",
    "| `torch.bool`     | -              | 1-bit    | Boolean values (True/False)                      | Masks, conditions                              |\n",
    "| `torch.bfloat16` | -              | 16-bit   | Brain floating point (better range than float16) | Training on TPUs / some GPUs                   |\n",
    "\n",
    "---\n",
    "\n",
    "✅ **Key notes**:\n",
    "\n",
    "* **Default**: `torch.float32`.\n",
    "* **Best for deep learning**: Mix of `torch.float16` (for speed) and `torch.float32` (for stability).\n",
    "* **GPU**: Use `tensor.to(\"cuda\")` for GPU acceleration.\n",
    "* **Precision tradeoff**: Higher precision → slower but more accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78ee2132-9691-4d56-ba09-561c9cf07b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "torch.float16\n",
      "torch.float64 tensor([3., 6., 9.], dtype=torch.float64)\n",
      "torch.int32\n",
      "torch.bool\n"
     ]
    }
   ],
   "source": [
    "# Default tensor (float32)\n",
    "float_32_tensor = torch.tensor([3.0, 6.0, 9.0])  \n",
    "print(float_32_tensor.dtype)  # torch.float32\n",
    "\n",
    "# Half precision (float16)\n",
    "float_16_tensor = torch.tensor([3.0, 6.0, 9.0], dtype=torch.float16)  \n",
    "print(float_16_tensor.dtype)  # torch.float16\n",
    "\n",
    "# Double precision (float64)\n",
    "float_64_tensor = torch.tensor([3.0, 6.0, 9.0], dtype=torch.float64)  \n",
    "print(float_64_tensor.dtype, float_64_tensor)  # torch.float64\n",
    "\n",
    "# Integer tensor (int32)\n",
    "int_tensor = torch.tensor([1, 2, 3], dtype=torch.int32)  \n",
    "print(int_tensor.dtype)  # torch.int32\n",
    "\n",
    "# Boolean tensor\n",
    "bool_tensor = torch.tensor([True, False, True])  \n",
    "print(bool_tensor.dtype)  # torch.bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "47292c80-3be1-4162-817d-34e0dbc92cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor: tensor([[0.1187, 0.0805, 0.0168, 0.0373],\n",
      "        [0.9154, 0.6223, 0.3162, 0.0125],\n",
      "        [0.7083, 0.7527, 0.5687, 0.2659]])\n",
      "Shape of the tensor: torch.Size([3, 4])\n",
      "Data type of the tensor: torch.float32\n",
      "Device which the data is stored on: cpu\n"
     ]
    }
   ],
   "source": [
    "rand_tensor = torch.rand(3, 4)\n",
    "\n",
    "print(f'Tensor: {rand_tensor}')\n",
    "print(f'Shape of the tensor: {rand_tensor.shape}')\n",
    "print(f'Data type of the tensor: {rand_tensor.dtype}')\n",
    "print(f'Device which the data is stored on: {rand_tensor.device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2eaf9234-4666-4956-8be0-756d41c245dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor: tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]]) \n",
      "\n",
      "First low: tensor([1., 1., 1., 1.]) \n",
      "\n",
      "first column: tensor([1., 1., 1., 1.])\n",
      "last column: tensor([1., 1., 1., 1.])\n",
      "Modified tensor now: \n",
      " tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.ones(4, 4)\n",
    "\n",
    "print(f'tensor: {tensor} \\n')\n",
    "print(f'First low: {tensor[0]} \\n')\n",
    "print(f'first column: {tensor[:, 1]}')\n",
    "print(f'last column: {tensor[..., -1]}')\n",
    "tensor[:, 1] = 0\n",
    "print(f'Modified tensor now: \\n {tensor}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe2f10b-c953-4a04-b8cc-f1d3110c7fed",
   "metadata": {},
   "source": [
    "### Basic Operations \n",
    "Let's start with a few of the fundamental operations, addition (+), subtraction (-), mutliplication (*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5cb7fbe8-131b-458c-a821-446213165fd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([11, 12, 13])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.tensor([1, 2, 3])\n",
    "tensor + 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f972c589-30d2-4de3-a1b1-2e20148108ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([11, 12, 13])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor + 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "524212fd-fcea-47d2-863e-6f8be3c7d2d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10, 20, 30])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor * 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af330a5f-072b-4dac-acd7-afccc885c461",
   "metadata": {},
   "source": [
    "# 🧮 Matrix Multiplication in PyTorch\n",
    "\n",
    "Matrix multiplication is **one of the most fundamental operations** in machine learning and deep learning, powering neural networks, embeddings, and transformations.\n",
    "\n",
    "In PyTorch, matrix multiplication can be done using:\n",
    "\n",
    "* `torch.matmul()`\n",
    "* The `@` operator (shortcut for matrix multiplication)\n",
    "\n",
    "---\n",
    "\n",
    "## 🔑 Rules of Matrix Multiplication\n",
    "\n",
    "When multiplying two matrices (or tensors):\n",
    "\n",
    "1. **Inner dimensions must match**\n",
    "\n",
    "   * If one matrix has shape `(m, n)` and the other `(n, p)`, then multiplication is possible.\n",
    "   * Example: `(2, 3) @ (3, 2)` works, but `(3, 2) @ (3, 2)` does not.\n",
    "\n",
    "2. **The resulting matrix shape comes from the outer dimensions**\n",
    "\n",
    "   * Example: `(2, 3) @ (3, 2)` results in `(2, 2)`\n",
    "   * Example: `(3, 2) @ (2, 3)` results in `(3, 3)`\n",
    "\n",
    "👉 Think of it as: **inner dimensions collapse, outer dimensions remain.**\n",
    "\n",
    "---\n",
    "\n",
    "## ✨ Element-wise vs Matrix Multiplication\n",
    "\n",
    "It is important to distinguish between two types of multiplication:\n",
    "\n",
    "* **Element-wise multiplication (`*`)**\n",
    "  Each entry in one matrix is multiplied by the corresponding entry in the other.\n",
    "  Requires matrices of the same shape (or broadcastable).\n",
    "\n",
    "  * Example: `(2, 2) * (2, 2)` → `(2, 2)`\n",
    "\n",
    "* **Matrix multiplication (`@` or `matmul`)**\n",
    "  Uses linear algebra rules. Inner dimensions must align, and the result takes the shape of the outer dimensions.\n",
    "\n",
    "  * Example: `(2, 3) @ (3, 2)` → `(2, 2)`\n",
    "\n",
    "---\n",
    "\n",
    "## 🧩 Why is this important in Deep Learning?\n",
    "\n",
    "* **Linear layers (fully connected layers)** are matrix multiplications between:\n",
    "\n",
    "  * Input data (batch size × input features)\n",
    "  * Weights (input features × output features)\n",
    "  * Output (batch size × output features)\n",
    "\n",
    "* **Transformations in neural networks** (embeddings, convolutions in linear form, attention in Transformers) are built on matrix multiplication.\n",
    "\n",
    "* **GPUs** excel at matrix multiplication, making it the backbone of efficient deep learning computations.\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 Summary Table\n",
    "\n",
    "| Operation                       | Symbol                | Shape Rule                                                 | Example                      |\n",
    "| ------------------------------- | --------------------- | ---------------------------------------------------------- | ---------------------------- |\n",
    "| **Element-wise multiplication** | `*`                   | Shapes must match (or be broadcastable)                    | `(2, 2) * (2, 2)` → `(2, 2)` |\n",
    "| **Matrix multiplication**       | `@` or `torch.matmul` | Inner dimensions must match; result takes outer dimensions | `(2, 3) @ (3, 2)` → `(2, 2)` |\n",
    "\n",
    "---\n",
    "\n",
    "📚 **Resources**\n",
    "\n",
    "* [PyTorch Documentation — `torch.matmul`](https://pytorch.org/docs/stable/generated/torch.matmul.html)\n",
    "* [Wikipedia: Matrix Multiplication](https://en.wikipedia.org/wiki/Matrix_multiplication)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "913b575f-fa66-44ec-895b-41b6c28f03bb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3]), 1)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.tensor([1, 2, 5])\n",
    "tensor.shape, tensor.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c974fe94-85d6-41df-8af4-e2500a7dc691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  4, 25])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor * tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a5b1e206-d443-404a-ad53-bc9121d1889b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(30)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor @ tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "51677f66-c493-4088-ba1e-ee4a3f768175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.02 ms, sys: 833 µs, total: 1.85 ms\n",
      "Wall time: 1.21 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(30)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Matrix multiplication by hand \n",
    "# (avoid doing operations with for loops at all cost, they are computationally expensive)\n",
    "value = 0\n",
    "for i in range(len(tensor)):\n",
    "  value += tensor[i] * tensor[i]\n",
    "value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "01700711-3e0a-4d31-adda-8e2252a582b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 269 µs, sys: 169 µs, total: 438 µs\n",
      "Wall time: 5.97 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(30)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "torch.matmul(tensor, tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9e8a87a0-4366-4dbf-9e99-4e97622c00a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 27.,  30.,  33.],\n",
       "        [ 61.,  68.,  75.],\n",
       "        [ 95., 106., 117.]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shapes need to be in the right way  \n",
    "tensor_A = torch.tensor([[1, 2],\n",
    "                         [3, 4],\n",
    "                         [5, 6]], dtype=torch.float32)\n",
    "\n",
    "# print(f'dim of tensor_A: {tensor_A.shape}')\n",
    "\n",
    "tensor_B = torch.tensor([[7, 10],\n",
    "                         [8, 11], \n",
    "                         [9, 12]], dtype=torch.float32)\n",
    "\n",
    "# print(f'dim of tensor_A: {tensor_B.shape}')\n",
    "\n",
    "\n",
    "matmul = torch.matmul(tensor_A, tensor_B.T)\n",
    "matmul"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884b98f1-8ab2-4ecb-9d8f-6096ccbd9bb4",
   "metadata": {},
   "source": [
    "> **Note:**  \n",
    "> A matrix multiplication like this is also referred to as the **dot product** of two matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b553989e-9a06-4975-9bf5-a86492e76838",
   "metadata": {},
   "source": [
    "# 🔗 Neural Networks and Matrix Multiplication\n",
    "\n",
    "At their core, **neural networks are built on matrix multiplications and dot products.**\n",
    "\n",
    "One of the most common building blocks in PyTorch is the **`torch.nn.Linear()` module**, also called a:\n",
    "\n",
    "* **Feed-forward layer**\n",
    "* **Fully connected layer (FC layer)**\n",
    "\n",
    "This layer essentially performs a **linear transformation** of the input data.\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ How it Works\n",
    "\n",
    "The formula behind a linear layer is:\n",
    "\n",
    "$$\n",
    "y = xA^T + b\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* **x** → The **input** to the layer (a vector or matrix).\n",
    "* **A** → The **weights matrix** created by the layer.\n",
    "\n",
    "  * Starts out as random numbers.\n",
    "  * Gets adjusted during training as the model learns patterns in the data.\n",
    "  * Notice the **transpose (T)** because the multiplication aligns dimensions properly.\n",
    "* **b** → The **bias term**, which allows shifting the output and gives the model more flexibility.\n",
    "* **y** → The **output**, which is a transformed version of the input.\n",
    "\n",
    "---\n",
    "\n",
    "## 📐 Connection to High School Algebra\n",
    "\n",
    "This is just a higher-dimensional version of the simple **linear function** you may have seen before:\n",
    "\n",
    "$$\n",
    "y = mx + b\n",
    "$$\n",
    "\n",
    "* Here, **m** is the slope (weights in neural networks).\n",
    "* **b** is the bias (same as in neural networks).\n",
    "* The difference is that in deep learning, instead of a single line, we’re working with **matrices and vectors** to represent higher-dimensional relationships.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔑 Why This Matters in Deep Learning\n",
    "\n",
    "* Every **fully connected layer** in a neural network is a **matrix multiplication + bias addition**.\n",
    "* By stacking many layers, neural networks can represent complex, non-linear patterns.\n",
    "* Training adjusts the weights (**A**) and biases (**b**) to minimize the difference between predicted output and actual data.\n",
    "\n",
    "---\n",
    "\n",
    "✅ **Summary:**\n",
    "`torch.nn.Linear()` = **Matrix Multiplication (input × weights) + Bias**\n",
    "\n",
    "This is the foundation of most deep learning architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c7da849e-b771-4b98-905f-4f066b6399a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "linear = torch.nn.Linear(in_features=2, out_features=6)\n",
    "\n",
    "x = tensor_A\n",
    "output = linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1a6aa74e-f674-406d-83af-1befb31928f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([3, 2])\n",
      "\n",
      "Output:\n",
      "tensor([[2.2368, 1.2292, 0.4714, 0.3864, 0.1309, 0.9838],\n",
      "        [4.4919, 2.1970, 0.4469, 0.5285, 0.3401, 2.4777],\n",
      "        [6.7469, 3.1648, 0.4224, 0.6705, 0.5493, 3.9716]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "\n",
      "Output shape: torch.Size([3, 6])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input shape: {x.shape}\\n\")\n",
    "print(f\"Output:\\n{output}\\n\\nOutput shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ce669783-01e0-4131-a2d2-844f953eb3f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the data type: torch.int64\n",
      "Minimum: 0\n",
      "Maximum: 0\n",
      "Mean: 0.0\n",
      "Sum: 0\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(0, 100, 100)\n",
    "print(f'the data type: {x.dtype}')\n",
    "print(f\"Minimum: {x.min()}\")\n",
    "print(f\"Maximum: {x.max()}\")\n",
    "# print(f\"Mean: {x.mean()}\") # this will error\n",
    "print(f\"Mean: {x.type(torch.float32).mean()}\") # won't work without float datatype\n",
    "print(f\"Sum: {x.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4ac23e04-9141-4b97-bd6f-3a5bd4bc6e32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.arange(10., 100., 10.)\n",
    "tensor.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "680af6f4-d37d-4de9-826d-285abb6b3ada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10., 20., 30., 40., 50., 60., 70., 80., 90.], dtype=torch.float16)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_float16 = tensor.type(torch.float16)\n",
    "tensor_float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6995d21f-5e49-4d85-88ef-83722a33bdc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10, 20, 30, 40, 50, 60, 70, 80, 90], dtype=torch.int8)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_int8 = tensor.type(torch.int8)\n",
    "tensor_int8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976b508a-2ecb-4bed-9bff-625fb4620d52",
   "metadata": {},
   "source": [
    "### 🔹 Reshaping, stacking, squeezing, and unsqueezing tensors\n",
    "\n",
    "* **torch.reshape(input, shape)**\n",
    "  Changes the arrangement of data into a new shape.\n",
    "  Example: `(2, 3)` → reshape → `(3, 2)`\n",
    "  (same 6 elements, just rearranged).\n",
    "\n",
    "* **Tensor.view(shape)**\n",
    "  Same as reshape but it shares the same data as the original tensor.\n",
    "  Example: `(4, 2)` → view → `(2, 4)`\n",
    "\n",
    "* **torch.stack(tensors, dim=0)**\n",
    "  Combines multiple tensors of the same size by adding a new dimension.\n",
    "  Example: three tensors of shape `(2, 2)` → stack → `(3, 2, 2)`\n",
    "\n",
    "* **torch.squeeze(input)**\n",
    "  Removes dimensions of size 1.\n",
    "  Example: `(1, 3, 1, 5)` → squeeze → `(3, 5)`\n",
    "\n",
    "* **torch.unsqueeze(input, dim)**\n",
    "  Adds a new dimension of size 1.\n",
    "  Example: `(3, 5)` → unsqueeze at dim=0 → `(1, 3, 5)`\n",
    "  Example: `(3, 5)` → unsqueeze at dim=2 → `(3, 5, 1)`\n",
    "\n",
    "* **torch.permute(input, dims)**\n",
    "  Rearranges the order of dimensions.\n",
    "  Example: `(2, 3, 5)` → permute to `(3, 5, 2)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3c8ecbf2-843b-4951-bd07-f8676883ee4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([9]), tensor([1., 2., 3., 4., 5., 6., 7., 8., 9.]), torch.float32)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(1., 10.)\n",
    "x.shape, x, x.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "fdd9088c-7ed9-4824-bbfa-25786411a92f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 2., 3.],\n",
       "         [4., 5., 6.],\n",
       "         [7., 8., 9.]]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.reshape(1, 3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f47e273c-c4a9-49e3-9fff-ea065747721e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 2., 3., 4., 5., 6., 7., 8., 9.]]), torch.Size([1, 9]))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = x.view(1, 9)\n",
    "z, z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e4d2684e-ae21-4375-aab3-3cea7c07f850",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z[:, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6ead8233-2144-4389-b98a-911217b106e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 9]),\n",
       " tensor([[1., 2., 3., 4., 5., 6., 7., 8., 9.],\n",
       "         [1., 2., 3., 4., 5., 6., 7., 8., 9.],\n",
       "         [1., 2., 3., 4., 5., 6., 7., 8., 9.],\n",
       "         [1., 2., 3., 4., 5., 6., 7., 8., 9.]]))"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_stacked = torch.vstack([x, x, x, x])\n",
    "x_stacked.shape, x_stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ec7531e7-47d9-43f5-8d76-673400e3bc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_reshaped_by1 = x.reshape(1, 9)\n",
    "x_reshaped_by3 = x.reshape(3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "6e722706-0729-4954-b993-9f0635e3bec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3., 4., 5., 6., 7., 8., 9.])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_reshaped_by1.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f0d9b146-5540-4432-a27c-40e48a1cfdbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_reshaped_by3.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "2a9ae24b-92bd-4241-bb79-903e4c147ba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 1, 3]),\n",
       " tensor([[[1., 2., 3.]],\n",
       " \n",
       "         [[4., 5., 6.]],\n",
       " \n",
       "         [[7., 8., 9.]]]))"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_unsquezed = x_reshaped_by3.unsqueeze(dim=1)\n",
    "x_unsquezed.shape, x_unsquezed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "17d37e70-6ade-4e61-9ba2-86fa6d926c3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 3])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_reshaped_by3.unsqueeze(dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec9957d-da64-4d0a-bdc8-20212a1d847b",
   "metadata": {},
   "source": [
    " ### Understanding `dim=0`\n",
    "\n",
    "* `dim=0` refers to the **first axis** of a tensor.\n",
    "* In a **2D matrix**, this is the **rows** axis.\n",
    "* When you perform an operation along `dim=0`, PyTorch combines values **across rows**, while keeping columns separate.\n",
    "\n",
    "**Example:**\n",
    "Suppose you have a tensor of shape `(3, 4)` → 3 rows, 4 columns.\n",
    "\n",
    "```\n",
    "[\n",
    " [1, 2, 3, 4],      <- row 0\n",
    " [5, 6, 7, 8],      <- row 1\n",
    " [9, 10, 11, 12]    <- row 2\n",
    "]\n",
    "```\n",
    "\n",
    "* If you **sum over `dim=0`**, you’re adding values down each column:\n",
    "  Result: `[1+5+9, 2+6+10, 3+7+11, 4+8+12]` → `[15, 18, 21, 24]`\n",
    "* Shape changes from `(3, 4)` → `(4,)`\n",
    "\n",
    "So, `dim=0` means: **operate top-to-bottom across rows**.\n",
    "\n",
    "---\n",
    "\n",
    "### Understanding `dim=1`\n",
    "\n",
    "* `dim=1` refers to the **second axis** of a tensor.\n",
    "* In a **2D matrix**, this is the **columns** axis.\n",
    "* When you perform an operation along `dim=1`, PyTorch combines values **across columns**, while keeping rows separate.\n",
    "\n",
    "**Example:**\n",
    "Same matrix `(3, 4)`:\n",
    "\n",
    "```\n",
    "[\n",
    " [1, 2, 3, 4],      <- row 0\n",
    " [5, 6, 7, 8],      <- row 1\n",
    " [9, 10, 11, 12]    <- row 2\n",
    "]\n",
    "```\n",
    "\n",
    "* If you **sum over `dim=1`**, you’re adding values across each row:\n",
    "  Row 0 → `1+2+3+4 = 10`\n",
    "  Row 1 → `5+6+7+8 = 26`\n",
    "  Row 2 → `9+10+11+12 = 42`\n",
    "\n",
    "Result: `[10, 26, 42]`\n",
    "\n",
    "* Shape changes from `(3, 4)` → `(3,)`\n",
    "\n",
    "So, `dim=1` means: **operate left-to-right across columns**.\n",
    "\n",
    "---\n",
    "\n",
    "### Extending Beyond 2D\n",
    "\n",
    "* **1D Tensor (vector):** Only `dim=0` exists.\n",
    "* **3D Tensor (e.g., image batches):**\n",
    "\n",
    "  * `dim=0` → batch (different images).\n",
    "  * `dim=1` → rows/height.\n",
    "  * `dim=2` → columns/width.\n",
    "\n",
    "Each new dimension just adds another “direction” to your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "6e32ff68-6680-43d2-8b1d-6af0c3169207",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_original = torch.rand(size=(224, 224, 3))\n",
    "\n",
    "x_permuted = x_original.permute(2, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "452088ce-c34b-4651-8a69-ba7e04bb2624",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 224, 224])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_permuted.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33622a86-82b2-49ae-8ef8-5ed22f815b3e",
   "metadata": {},
   "source": [
    "# PyTorch Tensors & NumPy\n",
    "\n",
    "Since **NumPy** is one of the most popular Python libraries for numerical computing, PyTorch provides built-in functionality to easily interact with it. This makes it convenient to switch between the two libraries depending on what operations you need.\n",
    "\n",
    "The two most important methods are:\n",
    "\n",
    "* **`torch.from_numpy(ndarray)`**\n",
    "  Converts a **NumPy array** into a **PyTorch tensor**.\n",
    "  This is useful when you already have data in NumPy and want to use PyTorch features (like GPU acceleration or autograd).\n",
    "\n",
    "* **`torch.Tensor.numpy()`**\n",
    "  Converts a **PyTorch tensor** into a **NumPy array**.\n",
    "  This is helpful when you want to leverage NumPy’s rich ecosystem of functions, visualization tools, or integrations with other Python libraries.\n",
    "\n",
    "⚡ Note: Both objects share the **same underlying memory** when possible. This means changes in the tensor may reflect in the NumPy array (and vice versa)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "a037be30-daa8-44cb-b380-7d8be4c9ad09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 2., 3., 4., 5., 6., 7.], dtype=torch.float64),\n",
       " array([1., 2., 3., 4., 5., 6., 7.]))"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array = np.arange(1.0, 8.)\n",
    "\n",
    "tensor = torch.from_numpy(array)\n",
    "\n",
    "numpy_tensor = tensor.numpy()\n",
    "\n",
    "tensor, numpy_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2d0f3e-de52-4c82-9640-2446a553cfc3",
   "metadata": {},
   "source": [
    "# Reproducibility (Taking the Random Out of Random)\n",
    "\n",
    "As you dive deeper into **neural networks and machine learning**, you’ll notice how much randomness plays a role.\n",
    "\n",
    "Of course, this is not *true randomness*, but rather **pseudorandomness**. Computers are deterministic by design (each step is predictable), so the randomness they generate is simulated.\n",
    "\n",
    "---\n",
    "\n",
    "### Why does randomness matter in neural networks?\n",
    "\n",
    "Neural networks often **start with random numbers** to initialize their parameters (like weights). These numbers initially describe patterns in data very poorly. Through training, the network applies many **tensor operations** and adjustments, improving these random numbers step by step until they represent useful patterns.\n",
    "\n",
    "In short:\n",
    "random numbers → tensor operations → better numbers → repeat until improved\n",
    "\n",
    "---\n",
    "\n",
    "### The problem with randomness\n",
    "\n",
    "While randomness is powerful, it makes experiments harder to reproduce. For example:\n",
    "\n",
    "* You design a model that achieves a certain level of performance.\n",
    "* Your friend runs the same code but gets slightly different results.\n",
    "\n",
    "This inconsistency happens because of the randomness in initialization and other operations.\n",
    "\n",
    "---\n",
    "\n",
    "### The solution: Reproducibility\n",
    "\n",
    "Reproducibility means being able to **get the same (or very similar) results** when running the same code on different machines or at different times.\n",
    "\n",
    "It is crucial for:\n",
    "\n",
    "* **Experiment verification** – others can confirm your results.\n",
    "* **Debugging** – you can consistently test changes.\n",
    "* **Scientific progress** – reliable results are needed to build upon prior work.\n",
    "\n",
    "---\n",
    "\n",
    "## Best Practices for Reproducibility in PyTorch\n",
    "\n",
    "To reduce randomness and make experiments reproducible, you can follow these practices:\n",
    "\n",
    "1. **Set random seeds**\n",
    "\n",
    "   * PyTorch, NumPy, and Python’s `random` module all generate random numbers.\n",
    "   * Setting the same seed ensures they produce the same sequences each time.\n",
    "\n",
    "2. **Fix CUDA randomness** (when using GPUs)\n",
    "\n",
    "   * Some GPU operations are nondeterministic for efficiency reasons.\n",
    "   * You can enable deterministic algorithms in PyTorch to reduce this.\n",
    "\n",
    "3. **Control data shuffling**\n",
    "\n",
    "   * Data loaders often shuffle batches randomly during training.\n",
    "   * Fixing the random seed ensures the same batch order across runs.\n",
    "\n",
    "4. **Document your environment**\n",
    "\n",
    "   * Versions of PyTorch, CUDA, and other libraries can impact reproducibility.\n",
    "   * Always record software versions when running experiments.\n",
    "\n",
    "5. **Be aware of hardware differences**\n",
    "\n",
    "   * CPUs vs. GPUs, or even different GPU models, can sometimes lead to slight numerical differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "bfa895e6-deae-455c-819b-133cd53eb772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor A:\n",
      "tensor([[0.8892, 0.1317, 0.7164, 0.2706],\n",
      "        [0.6560, 0.6614, 0.3648, 0.1490],\n",
      "        [0.8693, 0.0294, 0.5449, 0.0794]])\n",
      "\n",
      "Tensor B:\n",
      "tensor([[0.2313, 0.2927, 0.3386, 0.3708],\n",
      "        [0.0831, 0.0944, 0.6568, 0.7708],\n",
      "        [0.1053, 0.1467, 0.1932, 0.9118]])\n",
      "\n",
      "Does Tensor A equal Tensor B? (anywhere)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False],\n",
       "        [False, False, False, False],\n",
       "        [False, False, False, False]])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_tensor_A = torch.rand(3, 4)\n",
    "random_tensor_B = torch.rand(3, 4)\n",
    "\n",
    "print(f\"Tensor A:\\n{random_tensor_A}\\n\")\n",
    "print(f\"Tensor B:\\n{random_tensor_B}\\n\")\n",
    "print(f\"Does Tensor A equal Tensor B? (anywhere)\")\n",
    "random_tensor_A == random_tensor_B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafc4233-256a-40fd-97f2-edae0b5e68c3",
   "metadata": {},
   "source": [
    "### 1️⃣ Randomness on a computer is fake\n",
    "\n",
    "Computers are **deterministic**, meaning if you give the same input, you always get the same output. True randomness doesn’t naturally exist for computers.\n",
    "\n",
    "So what computers do is generate **pseudorandom numbers**—numbers that **look random** but are actually calculated using a deterministic algorithm.\n",
    "\n",
    "---\n",
    "\n",
    "### 2️⃣ Seeds “flavor” the randomness\n",
    "\n",
    "The pseudorandom number generator (PRNG) works like this:\n",
    "\n",
    "```\n",
    "next_number = f(previous_number)\n",
    "```\n",
    "\n",
    "* It starts with a number called the **seed**.\n",
    "* From that seed, it deterministically generates a sequence of numbers that **looks random**.\n",
    "* If you start from the same seed, the sequence is exactly the same every time.\n",
    "\n",
    "Think of it like starting a song from the same exact point: the melody will always be identical.\n",
    "\n",
    "---\n",
    "\n",
    "### 3️⃣ Why 42 is used\n",
    "\n",
    "* There’s **nothing magical** about 42 mathematically.\n",
    "* It became a **popular convention** because of *The Hitchhiker’s Guide to the Galaxy*.\n",
    "* Any integer could be used (1, 123, 2025…) and it would work the same for reproducibility.\n",
    "\n",
    "It’s purely for **convenience and culture**, not for randomness itself.\n",
    "\n",
    "---\n",
    "\n",
    "### 4️⃣ Why you care about seeds in PyTorch\n",
    "\n",
    "Imagine you’re training a neural network:\n",
    "\n",
    "* If you initialize weights randomly without a seed, every run gives slightly different results.\n",
    "* This makes debugging very hard, because you can’t tell if a change in output is due to your code or just random chance.\n",
    "\n",
    "Setting a seed with `torch.manual_seed(seed)`:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(42)\n",
    "a = torch.rand(3)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "b = torch.rand(3)\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "```\n",
    "\n",
    "**Output will always be the same**, e.g.:\n",
    "\n",
    "```\n",
    "tensor([0.3745, 0.9507, 0.7320])\n",
    "tensor([0.3745, 0.9507, 0.7320])\n",
    "```\n",
    "\n",
    "Even though the numbers are “random-looking,” using the same seed guarantees reproducibility.\n",
    "\n",
    "---\n",
    "\n",
    "### 5️⃣ Analogy: baking cookies\n",
    "\n",
    "* Seed = recipe\n",
    "* PRNG = oven\n",
    "* Random numbers = cookies\n",
    "\n",
    "If you use the **same recipe (seed)**, you’ll always get the **same batch of cookies**, even if they look random (different shapes, chocolate chips, etc.)\n",
    "\n",
    "---\n",
    "\n",
    "✅ **Key points:**\n",
    "\n",
    "1. Computers can’t generate true randomness.\n",
    "2. Seeds are just starting points for a pseudorandom sequence.\n",
    "3. 42 is a cultural convention, not special mathematically.\n",
    "4. Using the same seed makes your “random” outputs reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "87409e99-53e6-4b88-9bbf-8db058e36d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "218b1038-2395-4725-93ee-1ea08c52e723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor C:\n",
      "tensor([[0.8823, 0.9150, 0.3829, 0.9593],\n",
      "        [0.3904, 0.6009, 0.2566, 0.7936],\n",
      "        [0.9408, 0.1332, 0.9346, 0.5936]])\n",
      "\n",
      "Tensor D:\n",
      "tensor([[0.8823, 0.9150, 0.3829, 0.9593],\n",
      "        [0.3904, 0.6009, 0.2566, 0.7936],\n",
      "        [0.9408, 0.1332, 0.9346, 0.5936]])\n",
      "\n",
      "Does Tensor C equal Tensor D? (anywhere)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True],\n",
       "        [True, True, True, True],\n",
       "        [True, True, True, True]])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RANDOM_SEED = 42\n",
    "\n",
    "torch.manual_seed(seed=RANDOM_SEED)\n",
    "random_tensor_C = torch.rand(3, 4)\n",
    "\n",
    "torch.manual_seed(seed=RANDOM_SEED)\n",
    "random_tensor_D = torch.rand(3, 4)\n",
    "\n",
    "print(f\"Tensor C:\\n{random_tensor_C}\\n\")\n",
    "print(f\"Tensor D:\\n{random_tensor_D}\\n\")\n",
    "print(f\"Does Tensor C equal Tensor D? (anywhere)\")\n",
    "\n",
    "random_tensor_C == random_tensor_D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d762bb96-ca63-4f3f-9311-6ca4baa3e34a",
   "metadata": {},
   "source": [
    "## 1️⃣ The basic idea\n",
    "\n",
    "A **PRNG** is an algorithm that produces a sequence of numbers that:\n",
    "\n",
    "* Looks random\n",
    "* Can be **reproduced** if you know the starting point (the seed)\n",
    "\n",
    "It’s called **pseudo** because it’s **deterministic**—the same input always gives the same output.\n",
    "\n",
    "Think of it like a really complicated math function that takes a number (seed) and churns out “random-looking” numbers.\n",
    "\n",
    "---\n",
    "\n",
    "## 2️⃣ How it works internally (simplified)\n",
    "\n",
    "Most PRNGs follow this pattern:\n",
    "\n",
    "```\n",
    "state = seed\n",
    "while generating numbers:\n",
    "    state = f(state)      # a deterministic function\n",
    "    output = transform(state)\n",
    "    yield output\n",
    "```\n",
    "\n",
    "Where:\n",
    "\n",
    "* `state` = internal memory of the generator\n",
    "* `f(state)` = mathematical function that scrambles the state\n",
    "* `transform(state)` = converts the state into a number between 0 and 1\n",
    "\n",
    "---\n",
    "\n",
    "### Example: Linear Congruential Generator (LCG)\n",
    "\n",
    "One of the simplest PRNGs is the **LCG**, used in many languages:\n",
    "\n",
    "```\n",
    "X_(n+1) = (a * X_n + c) % m\n",
    "```\n",
    "\n",
    "Where:\n",
    "\n",
    "* `X_n` = current state\n",
    "* `a`, `c`, `m` = carefully chosen constants\n",
    "* `% m` = modulo operation\n",
    "\n",
    "Then you normalize the number:\n",
    "\n",
    "```\n",
    "random_number = X_n / m   # now it’s between 0 and 1\n",
    "```\n",
    "\n",
    "**Step by step**:\n",
    "\n",
    "1. Pick a seed `X_0`\n",
    "2. Apply the formula to get `X_1`\n",
    "3. Output `X_1 / m` as your “random” number\n",
    "4. Repeat for `X_2, X_3, ...`\n",
    "\n",
    "Even though this is purely deterministic, the sequence **appears random**.\n",
    "\n",
    "---\n",
    "\n",
    "## 3️⃣ Why changing the seed gives different sequences\n",
    "\n",
    "* The seed is just `X_0`.\n",
    "* Even changing it by 1 gives a completely different sequence because the function `f(state)` **amplifies differences** over iterations.\n",
    "* Good PRNGs make it so that sequences from similar seeds are **unpredictable and statistically independent**.\n",
    "\n",
    "---\n",
    "\n",
    "## 4️⃣ Why it looks random\n",
    "\n",
    "PRNGs are designed to pass statistical tests:\n",
    "\n",
    "* Uniformity: all numbers are equally likely\n",
    "* Independence: one number doesn’t predict the next\n",
    "* Long period: the sequence doesn’t repeat for a huge number of numbers\n",
    "\n",
    "Modern PRNGs, like **Mersenne Twister** (used in Python and PyTorch), have:\n",
    "\n",
    "* Period = 2^19937 − 1 (a HUGE number)\n",
    "* Very uniform distribution\n",
    "* Good randomness properties for simulations and machine learning\n",
    "\n",
    "---\n",
    "\n",
    "## 5️⃣ Key points about PRNGs\n",
    "\n",
    "1. **Deterministic** → reproducible if you know the seed\n",
    "2. **Any integer seed works** → it just sets the starting state\n",
    "3. **Not truly random** → computers need hardware RNGs for real entropy\n",
    "4. **Good PRNGs** → sequences pass randomness tests and appear unpredictable\n",
    "\n",
    "---\n",
    "\n",
    "### Analogy\n",
    "\n",
    "Imagine a **huge maze with numbered doors**:\n",
    "\n",
    "* Seed = starting door\n",
    "* PRNG = a rule that tells you which door to go to next\n",
    "* Numbers you see = output of PRNG\n",
    "\n",
    "Even though the maze has a fixed layout, if you start at **the same door (seed)**, your path (sequence of numbers) is exactly the same. Start at a **different door**, and your path looks completely different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "575d9e1b-fe5a-4c74-b161-b815fef6760b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: nvidia-smi\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "e6c13987-1894-489d-97bd-17971e50ffcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.backends.mps.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "da26c80e-603b-4f91-9a36-0b12f772bbdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fce662-2430-4edb-bb7f-3f1f7627f527",
   "metadata": {},
   "source": [
    "### Putting tensors (and models) on the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "63bd8d7b-3613-4bb3-8232-a33f945ed0ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor: tensor([1, 2, 3, 4]) \n",
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.tensor([1, 2, 3, 4])\n",
    "\n",
    "print(f\"Tensor: {tensor} \\nDevice: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "66347c0c-b027-4222-971a-bd7f9206792d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4], device='mps:0')"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Moving tensor to GPU \n",
    "tensor_on_gpu = tensor.to(device)\n",
    "\n",
    "tensor_on_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "18b520b6-c4e4-408b-a4b1-482d29020e33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_on_gpu.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "6865b63f-0683-4632-83ca-00697561d79b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4], device='mps:0')"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_on_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f25c7e1-4a02-4b0a-a9e5-cf4453043183",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (env_pytorch)",
   "language": "python",
   "name": "env_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
